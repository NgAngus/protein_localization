{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook engineers features, fills missing values, and provides a way to impute missing values of features using intraclass modes.\n",
    "\n",
    "To run this on kaggle:\n",
    "* Simply upload the notebook, from `File > Open Notebook`\n",
    "* Set the global variable `LOCAL = False`.\n",
    "* You will also need a local dependency `data_tools.py` as a utility script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LOCAL = True\n",
    "data_fpath = '../data/raw/' if LOCAL else '/kaggle/input/protein-localization/'\n",
    "out_fpath = '../data/intermediate/' if LOCAL else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import data_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Types\n",
    "There are a few main “types” of features available, listed here:\n",
    "* ESSENTIAL\n",
    "* CLASS\n",
    "* COMPLEX\n",
    "* PHENOTYPE\n",
    "* MOTIF\n",
    "* Chromosome\n",
    "* NUM INTERACTING WITH FUNCTION (int)\n",
    "* INTERACTING PROTEIN type\n",
    "* INTERACTING PROTEIN corr (float)\n",
    "* Function\n",
    "* Localization\n",
    "\n",
    "Pretty much all are categorical except the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>protein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>essential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>class actin related proteins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class actins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>class adaptins</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0\n",
       "0                            protein\n",
       "1                          essential\n",
       "2  class actin related proteins     \n",
       "3                class actins       \n",
       "4                     class adaptins"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_descriptions_fpath = data_tools.field_descriptions_fpath\n",
    "fields = data_tools.parse_field_descriptions(field_descriptions_fpath)\n",
    "fields[[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype Specification\n",
    "def feat_dtype(col_num, ints, floats):\n",
    "    if col_num in ints:\n",
    "        return 'int'\n",
    "    if col_num in floats:\n",
    "        return 'float'\n",
    "    # We assume everything else is categorical\n",
    "    return 'category'\n",
    "\n",
    "float_indices = fields[0].str.contains(\"interacting protein\") & fields[0].str.contains(\"corr\")\n",
    "int_indices = fields[0].str.contains(\"num interacting\")\n",
    "\n",
    "### 444 := chromosome #, coerce to float, so we fill missing, then turn into category later\n",
    "float_feats = set(fields[[0]][float_indices].index) - {0, 2960} | {444}\n",
    "int_feats = set(fields[[0]][int_indices].index) - {0, 2960}\n",
    "\n",
    "dtypes = {col_num : feat_dtype(col_num, int_feats, float_feats) for col_num in range(1,2961)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data type dictionary so we can load it later when loading the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data type dictionary to pickle\n",
    "with open(f'{out_fpath}data_types_dict.pkl', 'wb') as handle:\n",
    "    pickle.dump(dtypes, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{out_fpath}data_types_dict.pkl', 'rb') as handle:\n",
    "    dtypes = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angus/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (444) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{data_fpath}train.csv\", header=None)\n",
    "df = df.replace(\"?\", np.nan)  # Replace ? mark with NaN\n",
    "df = df.astype(dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angus/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (524,720,768,1112,1182,1288,1302,1352,1354,1378,1434,1436,1488,1502,1504,1604,1608,1734,1838,1908,1914,1942,1996,2246,2270,2328,2460,2514,2576,2620,2724,2758,2930) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.read_csv(f\"{data_fpath}test.csv\", header=None)\n",
    "testdf = testdf.replace(\"?\", np.nan)  # Replace ? mark with NaN\n",
    "dtypes.pop(2960, None)  # Pop target from data types\n",
    "testdf = testdf.astype(dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        44\n",
       "444       1\n",
       "534       4\n",
       "566       1\n",
       "674       1\n",
       "       ... \n",
       "2940    862\n",
       "2941    862\n",
       "2942    862\n",
       "2943    862\n",
       "2944    862\n",
       "Length: 815, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Values\n",
    "df[df.isna().any()[lambda x: x].index].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        17\n",
       "444      59\n",
       "463       1\n",
       "475       1\n",
       "483       1\n",
       "       ... \n",
       "2955    381\n",
       "2956    381\n",
       "2957    381\n",
       "2958    381\n",
       "2959    381\n",
       "Length: 158, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Values\n",
    "testdf[testdf.isna().any()[lambda x: x].index].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that for column 1 and 444 which are fairly important features, the number of missing values in training data is fairly small. It is worth imputing them with the most common value in their class so we can use SMOTE later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_by_class_mode(df, col, target_col=2960):\n",
    "    '''Imputes a column with missing values by using\n",
    "    the mode of that feature within the class.\n",
    "    df : full dataframe with target_col\n",
    "    col : column to impute\n",
    "    '''\n",
    "    # Create a map of class to mode of feature in col\n",
    "    mode_map = df.loc[:, [col, target_col]].groupby(target_col)[col].agg(pd.Series.mode).to_dict()\n",
    "    # Make all values lists in case ties\n",
    "    mode_map = {k : np.asarray(v).tolist() for k, v in mode_map.items()}\n",
    "    mode_map = {k : [v] if not isinstance(v, list) else v for k, v in mode_map.items()}\n",
    "    # Make copy of column to impute\n",
    "    col_to_impute = df[col].copy()\n",
    "    # Identify rows with missing values\n",
    "    missing_idxs = col_to_impute.isna()[lambda x: x].index\n",
    "    col_to_impute.iloc[missing_idxs] = df.iloc[missing_idxs, target_col].apply(lambda x: choice(mode_map[x]))\n",
    "    return col_to_impute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[1] = impute_by_class_mode(df, 1)\n",
    "df[444] = impute_by_class_mode(df, 444)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534       4\n",
       "566       1\n",
       "674       1\n",
       "720       9\n",
       "732       1\n",
       "       ... \n",
       "2940    862\n",
       "2941    862\n",
       "2942    862\n",
       "2943    862\n",
       "2944    862\n",
       "Length: 813, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Values\n",
    "df[df.isna().any()[lambda x: x].index].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Interactions File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(910, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_colnames = ['protein1', 'protein2', 'type', 'strength']\n",
    "df2 = pd.read_csv(f\"{data_fpath}protein_interactions.csv\", header=None, names=interaction_colnames,\n",
    "    dtype={\n",
    "        'type' : 'category',\n",
    "    }\n",
    ")\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The strength values have a random period at\n",
    "# the end of the values preventing it from being parsed as numeric\n",
    "df2['strength'] = df2['strength'].str.rstrip('.').replace(\"?\", np.nan)\n",
    "df2['strength'] = pd.to_numeric(df2['strength'].str.rstrip('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein1</th>\n",
       "      <th>protein2</th>\n",
       "      <th>type</th>\n",
       "      <th>strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P238510</td>\n",
       "      <td>P239467</td>\n",
       "      <td>Genetic</td>\n",
       "      <td>0.252653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P235550</td>\n",
       "      <td>P239467</td>\n",
       "      <td>Physical</td>\n",
       "      <td>0.709248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P235621</td>\n",
       "      <td>P239467</td>\n",
       "      <td>Physical</td>\n",
       "      <td>-0.001239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P235265</td>\n",
       "      <td>P239467</td>\n",
       "      <td>Physical</td>\n",
       "      <td>0.482255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P234935</td>\n",
       "      <td>P234445</td>\n",
       "      <td>Physical</td>\n",
       "      <td>-0.460856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  protein1 protein2      type  strength\n",
       "0  P238510  P239467   Genetic  0.252653\n",
       "1  P235550  P239467  Physical  0.709248\n",
       "2  P235621  P239467  Physical -0.001239\n",
       "3  P235265  P239467  Physical  0.482255\n",
       "4  P234935  P234445  Physical -0.460856"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Interactions Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2486, 1243, 1243]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ppi := protein-protein interactions\n",
    "qry = fields[0].str.contains(\"interacting protein\")\n",
    "ppi_features = set(fields[[0]][qry].index) - {0}\n",
    "qry_corr = fields[0].str.contains(\"corr\")\n",
    "qry_type = fields[0].str.contains(\"type\")\n",
    "\n",
    "ppi_corr_features = set(fields[[0]][qry & qry_corr].index) - {0}\n",
    "ppi_type_features = set(fields[[0]][qry & qry_type].index) - {0}\n",
    "[len(x) for x in (ppi_features, ppi_corr_features, ppi_type_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_feat(protein : str):\n",
    "    '''E.g.: interacting protein p235094 corr'''\n",
    "    return f\"interacting protein {protein.lower()} corr\"\n",
    "\n",
    "def type_feat(protein : str):\n",
    "    '''E.g.: interacting protein p235094 corr'''\n",
    "    return f\"interacting protein {protein.lower()} type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interacting protein p239476 corr'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tools.feature_name(fields, 460)  # Example interaction feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from feature name to column\n",
    "feat_to_col = data_tools.feat_to_col_map(data_tools.field_descriptions_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "830"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_to_col[corr_feat('P238510')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interacting protein p235082 corr'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tools.feature_name(fields, 2940)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a way to fill the protein interaction cells with the type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This protein is not present in the dataset at all! So how can we use the PPI features? Possible features to engineer:\n",
    "* Sum/min/max/mean/#negof interactions corr\n",
    "* mode of interactions\n",
    "* percent of interactions that are genetic\n",
    "* meta feature: mode of the CLASS of proteins that interact (data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive Features from PPI Correlation\n",
    "df['interaction_sum'] = df.loc[:, ppi_corr_features].sum(axis=1)\n",
    "df['interaction_abs_sum'] = df.loc[:, ppi_corr_features].abs().sum(axis=1)\n",
    "df['interaction_mean'] = df.loc[:, ppi_corr_features].mean(axis=1)\n",
    "df['interaction_max'] = df.loc[:, ppi_corr_features].max(axis=1)\n",
    "df['interaction_max2'] = df.loc[:, ppi_corr_features].apply(lambda row: row.nlargest(2).values[-1], axis=1)\n",
    "df['interaction_max3'] = df.loc[:, ppi_corr_features].apply(lambda row: row.nlargest(3).values[-1], axis=1)\n",
    "df['interaction_min'] = df.loc[:, ppi_corr_features].min(axis=1)\n",
    "df['interaction_neg'] = df.loc[:, ppi_corr_features].lt(0).sum(axis=1)\n",
    "df['interaction_count'] = (df.loc[:, ppi_corr_features] != 0).sum(axis=1)\n",
    "df['interaction_count'] = df['interaction_count'] - min(df['interaction_count'])\n",
    "df['interaction_pos'] = df.loc[:, ppi_corr_features].gt(0).sum(axis=1)\n",
    "df['interaction_std'] = df.loc[:, ppi_corr_features].std(axis=1)\n",
    "df['interaction_skew'] = df.loc[:, ppi_corr_features].apply(\n",
    "    lambda row: sp.stats.skew(row, nan_policy='omit'), axis=1).astype(float)\n",
    "df['interaction_kurt'] = df.loc[:, ppi_corr_features].apply(\n",
    "    lambda row: sp.stats.kurtosis(row, nan_policy='omit'), axis=1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive Features from PPI Type\n",
    "df['ppi_genetic_count'] = df.loc[:, ppi_type_features].apply(\n",
    "    lambda row: row.astype(str).str.count(\"Genetic\").sum(), axis=1)\n",
    "df['ppi_physical_count'] = df.loc[:, ppi_type_features].apply(\n",
    "    lambda row: row.astype(str).str.count(\"Physical\").sum(), axis=1)\n",
    "df['ppi_gen_phys_count'] = df.loc[:, ppi_type_features].apply(\n",
    "    lambda row: row.astype(str).str.count(\"Genetic-Physical\").sum(), axis=1)\n",
    "df['ppi_genetic_physical_ratio'] = df['ppi_genetic_count'] / (1 + df['ppi_physical_count'])\n",
    "ppi_type_feats = ['ppi_genetic_count', 'ppi_physical_count', 'ppi_gen_phys_count']\n",
    "df['ppi_dom_type'] = df[ppi_type_feats].idxmax(axis='columns').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get proteins of highest correlation\n",
    "# Note that if all ties, we return the first index (460)\n",
    "# luckily this is zero for all proteins so works as placeholder\n",
    "df['ppi_max_corr'] = df[ppi_corr_features].idxmax(axis='columns').astype('category')\n",
    "df['ppi_min_corr'] = df[ppi_corr_features].idxmax(axis='columns').astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPI Network Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_numeric_dtype(df.dtypes['interaction_sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I suspect this will be the ultimate feature.\n",
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def ppi_to_localization(protein, target=2959):\n",
    "    '''This function takes a PPI feature and determines the\n",
    "    localization of the protein if it exists in training data.\n",
    "    \n",
    "    Note that within this notebook, this is still referencing the training dataframe.\n",
    "    '''\n",
    "    # Get name of feature corresnponding to protein (a ppi feature column idx)\n",
    "    full_feat_name = data_tools.feature_name(fields, protein)\n",
    "    # Extract the name of the protein from the feature name\n",
    "    ppi_name = full_feat_name.split()[-2].upper()  # Protein name\n",
    "    # Search the training data for the corresponding protein, then the target feature of it\n",
    "    target_value = df.loc[df[0] == ppi_name, target].values\n",
    "    if not target_value:\n",
    "        return 0 if is_numeric_dtype(df.dtypes[target]) else 'unknown'\n",
    "    return choice(target_value)  # Extract element from list (there's only one element)\n",
    "    # Old code\n",
    "    if not localizations:\n",
    "        return 0 if is_numeric_dtype(df.dtypes[target]) else 'unknown'\n",
    "    if return_mode:\n",
    "        mode = list(pd.Series(localizations).mode().values)\n",
    "        return choice(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_mode(A):\n",
    "    if not A:\n",
    "        return 'unknown'\n",
    "    return choice(pd.Series(A).astype(str).mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['ppi_strongest_protein'] = df['ppi_max_corr'].apply(ppi_to_localization).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the localization of the protein that has the strongest pos/neg corelation\n",
    "cols = df.loc[:,ppi_corr_features].columns  # correlation columns\n",
    "gt = df.loc[:,ppi_corr_features].apply(lambda x: x > 0)  # Interaction corr > 0\n",
    "lt = df.loc[:,ppi_corr_features].apply(lambda x: x < 0)  # Interaction corr < 0\n",
    "nonzero = gt | lt  # Interaction corr < 0 or > 0\n",
    "\n",
    "df['mode_localization_pos'] = gt.apply(\n",
    "    lambda x: localization_mode([ppi_to_localization(prot, 2960) for prot in cols[x.values]]),\n",
    "    axis=1).astype('category')\n",
    "df['mode_localization_neg'] = lt.apply(\n",
    "    lambda x: localization_mode([ppi_to_localization(prot, 2960) for prot in cols[x.values]]),\n",
    "    axis=1).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second degree protein interaction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mean(a):\n",
    "    if not a:\n",
    "        return 0\n",
    "    return np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second degree positive interaction values\n",
    "df['second_deg_interaction_sum_pos'] = gt.apply(\n",
    "    lambda x: sum([ppi_to_localization(prot, 'interaction_sum') for prot in cols[x.values]]), axis=1)\n",
    "df['second_deg_interaction_mean_pos'] = gt.apply(\n",
    "    lambda x: my_mean([ppi_to_localization(prot, 'interaction_mean') for prot in cols[x.values]]), axis=1).fillna(0)\n",
    "df['second_deg_interaction_max_pos'] = gt.apply(\n",
    "    lambda x: max([ppi_to_localization(prot, 'interaction_max') for prot in cols[x.values]], default=0), axis=1)\n",
    "\n",
    "# Second degree negative interaction values\n",
    "df['second_deg_interaction_sum_neg'] = lt.apply(\n",
    "    lambda x: sum([ppi_to_localization(prot, 'interaction_sum') for prot in cols[x.values]]), axis=1)\n",
    "df['second_deg_interaction_mean_neg'] = lt.apply(\n",
    "    lambda x: my_mean([ppi_to_localization(prot, 'interaction_mean') for prot in cols[x.values]]), axis=1).fillna(0)\n",
    "df['second_deg_interaction_max_neg'] = lt.apply(\n",
    "    lambda x: max([ppi_to_localization(prot, 'interaction_max') for prot in cols[x.values]], default=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an array B where B[i] is the percent representation of i in A\n",
    "def arr_percents(A):\n",
    "    N = len(A)\n",
    "    B = [0] * 14\n",
    "    if N == 0:\n",
    "        return B\n",
    "    counts = collections.Counter(A)  # also a defaultdict(int)\n",
    "    for i in range(14):\n",
    "        B[i] = counts[i] / N\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For positive correlations, let's create a column for 14 classes + unknown\n",
    "ppis_gt = gt.apply(lambda x: [ppi_to_localization(prot, 2960) for prot in cols[x.values]], axis=1)\n",
    "ppi_pos_pct_cols = [f\"ppi_pos_pct_{i}\" for i in range(14)]\n",
    "df_ppi_pos_pct = pd.DataFrame(ppis_gt.apply(arr_percents).tolist(), columns=ppi_pos_pct_cols)\n",
    "df = pd.concat((df, df_ppi_pos_pct), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For negative correlations, let's create a column for 14 classes + unknown\n",
    "ppis_lt = lt.apply(lambda x: [ppi_to_localization(prot, 2960) for prot in cols[x.values]], axis=1)\n",
    "ppi_neg_pct_cols = [f\"ppi_neg_pct_{i}\" for i in range(14)]\n",
    "df_ppi_neg_pct = pd.DataFrame(ppis_lt.apply(arr_percents).tolist(), columns=ppi_neg_pct_cols)\n",
    "df = pd.concat((df, df_ppi_neg_pct), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ALL correlations, let's create a column for 14 classes + unknown\n",
    "ppis = nonzero.apply(lambda x: [ppi_to_localization(prot, 2960) for prot in cols[x.values]], axis=1)\n",
    "ppi_pct_cols = [f\"ppi_all_pct_{i}\" for i in range(14)]\n",
    "df_ppi_all_pct = pd.DataFrame(ppis.apply(arr_percents).tolist(), columns=ppi_pct_cols)\n",
    "df = pd.concat((df, df_ppi_all_pct), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class correlation feature\n",
    "* The idea is every protein has some n number of surrounding proteins\n",
    "* Each one of these proteins has a correlation\n",
    "* I want to engineer a feature that sums the correlation by class group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an array B where B[i] is the sum of correlations of the nbrs of the ith class\n",
    "# payload is a tuple (row index, d)\n",
    "# Where d is a dictionary {correlation_column : class label of nbr protein}\n",
    "def class_corr_feat(payload, func=sum):\n",
    "    row_idx, d = payload\n",
    "    B = [0] * 14  # Return this\n",
    "    B_dict = collections.defaultdict(list)  # will use this to aggregate values of nbr classes\n",
    "    if len(d) == 0:\n",
    "        return B\n",
    "    for corr_col, label in d.items():\n",
    "        B_dict[label].append(df.iloc[row_idx, corr_col])\n",
    "    for i in range(len(B)):\n",
    "        B[i] = func(np.array(B_dict[i]))\n",
    "    return np.nan_to_num(np.array(B), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                (0, {})\n",
       "1                                                (1, {})\n",
       "2                                                (2, {})\n",
       "3                                (3, {658: 11, 1766: 7})\n",
       "4      (4, {736: 0, 830: 0, 1440: 0, 1790: 0, 1872: 0...\n",
       "                             ...                        \n",
       "857                    (857, {766: 3, 1114: 0, 1734: 3})\n",
       "858                                            (858, {})\n",
       "859                                            (859, {})\n",
       "860                                            (860, {})\n",
       "861                                     (861, {1466: 5})\n",
       "Length: 862, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I create a vector whose elements are a dict\n",
    "# This is a dict that gives the (row_index, corr_feature_column) : class of that feature\n",
    "nbr_classes = nonzero.apply(lambda x: (\n",
    "    x.name, {prot : ppi_to_localization(prot, 2960) for prot in cols[x.values]}), axis=1)\n",
    "nbr_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angus/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/angus/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:261: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "# Okay now we can generate these features that are functions\n",
    "# of correlations of the neighbours broken down by class\n",
    "df_nbr_corr_sum = nbr_classes.apply(lambda row: class_corr_feat(row, func=sum))\n",
    "df_nbr_corr_mean = nbr_classes.apply(lambda row: class_corr_feat(row, func=np.mean))\n",
    "df_nbr_corr_max = nbr_classes.apply(lambda row: class_corr_feat(row, func=lambda vals: max(vals, default=0)))\n",
    "df_nbr_corr_std = nbr_classes.apply(lambda row: class_corr_feat(row, func=np.std))\n",
    "df_nbr_corr_skew = nbr_classes.apply(\n",
    "    lambda row: class_corr_feat(row, func=lambda vals: sp.stats.skew(vals, nan_policy='omit')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nbr_corr_sum = pd.DataFrame(df_nbr_corr_sum.tolist(), columns=[f\"nbr_corr_sum_{i}\" for i in range(14)])\n",
    "df_nbr_corr_mean = pd.DataFrame(df_nbr_corr_mean.tolist(), columns=[f\"nbr_corr_mean_{i}\" for i in range(14)])\n",
    "df_nbr_corr_max = pd.DataFrame(df_nbr_corr_max.tolist(), columns=[f\"nbr_corr_max_{i}\" for i in range(14)])\n",
    "df_nbr_corr_std = pd.DataFrame(df_nbr_corr_std.tolist(), columns=[f\"nbr_corr_std_{i}\" for i in range(14)])\n",
    "df_nbr_corr_skew = pd.DataFrame(df_nbr_corr_skew.tolist(), columns=[f\"nbr_corr_skew_{i}\" for i in range(14)])\n",
    "df = pd.concat((\n",
    "    df,\n",
    "    df_nbr_corr_sum,\n",
    "    df_nbr_corr_mean,\n",
    "    df_nbr_corr_max,\n",
    "    df_nbr_corr_std,\n",
    "    df_nbr_corr_skew,\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motif Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = fields[0].str.contains(\"motif\")\n",
    "motif_feats = set(fields[[0]][qry].index)  # list of motif features\n",
    "df['motif_interactions'] = df.loc[:, motif_feats].applymap(lambda x: 0 if x == \"No\" else 1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Degree Motif Interactions of positive correlation\n",
    "# reminder: gt is the indicator dataframe of positive interaction correlations \n",
    "df['second_deg_motif_interaction_sum_pos'] = gt.apply(\n",
    "    lambda x: sum([ppi_to_localization(prot, 'motif_interactions') for prot in cols[x.values]]), axis=1)\n",
    "df['second_deg_motif_interaction_mean_pos'] = gt.apply(\n",
    "    lambda x: my_mean([ppi_to_localization(prot, 'interaction_mean') for prot in cols[x.values]]), axis=1).fillna(0)\n",
    "df['second_deg_motif_interaction_max_pos'] = gt.apply(\n",
    "    lambda x: max([ppi_to_localization(prot, 'interaction_max') for prot in cols[x.values]], default=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Degree Motif Interactions of negative correlation\n",
    "# reminder: lt is the indicator dataframe of negative interaction correlations \n",
    "df['second_deg_motif_interaction_sum_neg'] = lt.apply(\n",
    "    lambda x: sum([ppi_to_localization(prot, 'motif_interactions') for prot in cols[x.values]]), axis=1)\n",
    "df['second_deg_motif_interaction_mean_neg'] = lt.apply(\n",
    "    lambda x: my_mean([ppi_to_localization(prot, 'interaction_mean') for prot in cols[x.values]]), axis=1).fillna(0)\n",
    "df['second_deg_motif_interaction_max_neg'] = lt.apply(\n",
    "    lambda x: max([ppi_to_localization(prot, 'interaction_max') for prot in cols[x.values]], default=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Degree Motif Interactions of nonzero correlation\n",
    "# reminder: lt is the indicator dataframe of negative interaction correlations \n",
    "df['second_deg_motif_interaction_sum'] = nonzero.apply(\n",
    "    lambda x: sum([ppi_to_localization(prot, 'motif_interactions') for prot in cols[x.values]]), axis=1)\n",
    "df['second_deg_motif_interaction_mean'] = nonzero.apply(\n",
    "    lambda x: my_mean([ppi_to_localization(prot, 'interaction_mean') for prot in cols[x.values]]), axis=1).fillna(0)\n",
    "df['second_deg_motif_interaction_max'] = nonzero.apply(\n",
    "    lambda x: max([ppi_to_localization(prot, 'interaction_max') for prot in cols[x.values]], default=0), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(df, fields, dtypes, impute=False, target_col=None, seed=None):\n",
    "    '''Takes a DataFrame and returns features to pass into model.'''\n",
    "    # Handle Missing Values\n",
    "    # TODO: Try using zeros\n",
    "    # LightGBM should handle NasNs though\n",
    "    df = df.replace(\"?\", np.nan)  # Replace ? mark with NaN\n",
    "    \n",
    "    # Convert to correct data types\n",
    "    if target_col is None:\n",
    "        dtypes.pop(2960)  # labels aren't in test data\n",
    "    df = df.astype(dtypes)\n",
    "    \n",
    "    # Impute Missing Values\n",
    "    # I've selected these columns very carefully\n",
    "    impute_cols = [1, 444]\n",
    "    for col in impute_cols:\n",
    "        if impute:\n",
    "            # Impute the column\n",
    "            df[col] = impute_by_class_mode(df, col)\n",
    "        # Convert back to categorical\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    # Motif features\n",
    "    qry = fields[0].str.contains(\"motif\")\n",
    "    motif_feats = set(fields[[0]][qry].index)  # list of motif features\n",
    "    df['motif_interactions'] = df.loc[:, motif_feats].applymap(lambda x: 0 if x == \"No\" else 1).sum(axis=1)\n",
    "\n",
    "    # Identify columns corresponding to PPI features\n",
    "    # PPI := protein-protein interactions\n",
    "    qry = fields[0].str.contains(\"interacting protein\")\n",
    "    ppi_features = set(fields[[0]][qry].index) - {0}\n",
    "    qry_corr = fields[0].str.contains(\"corr\")\n",
    "    qry_type = fields[0].str.contains(\"type\")\n",
    "\n",
    "    ppi_corr_features = set(fields[[0]][qry & qry_corr].index) - {0}\n",
    "    ppi_type_features = set(fields[[0]][qry & qry_type].index) - {0}\n",
    "    \n",
    "    # Derive Features from PPI Correlation\n",
    "    df['interaction_sum'] = df.loc[:, ppi_corr_features].sum(axis=1)\n",
    "    df['interaction_abs_sum'] = df.loc[:, ppi_corr_features].abs().sum(axis=1)\n",
    "    df['interaction_mean'] = df.loc[:, ppi_corr_features].mean(axis=1)\n",
    "    df['interaction_max'] = df.loc[:, ppi_corr_features].max(axis=1)\n",
    "    df['interaction_max2'] = df.loc[:, ppi_corr_features].apply(lambda row: row.nlargest(2).values[-1], axis=1)\n",
    "    df['interaction_max3'] = df.loc[:, ppi_corr_features].apply(lambda row: row.nlargest(3).values[-1], axis=1)\n",
    "    df['interaction_min'] = df.loc[:, ppi_corr_features].min(axis=1)\n",
    "    df['interaction_neg'] = df.loc[:, ppi_corr_features].lt(0).sum(axis=1)\n",
    "    df['interaction_count'] = (df.loc[:, ppi_corr_features] != 0).sum(axis=1)\n",
    "    df['interaction_count'] = df['interaction_count'] - min(df['interaction_count'])\n",
    "    df['interaction_pos'] = df.loc[:, ppi_corr_features].gt(0).sum(axis=1)\n",
    "    df['interaction_std'] = df.loc[:, ppi_corr_features].std(axis=1)\n",
    "    df['interaction_skew'] = df.loc[:, ppi_corr_features].apply(\n",
    "        lambda row: sp.stats.skew(row, nan_policy='omit'), axis=1).astype(float)\n",
    "    df['interaction_kurt'] = df.loc[:, ppi_corr_features].apply(\n",
    "        lambda row: sp.stats.kurtosis(row, nan_policy='omit'), axis=1).astype(float)\n",
    "\n",
    "    # Derive Features from PPI Type\n",
    "    df['ppi_genetic_count'] = df.loc[:, ppi_type_features].apply(\n",
    "        lambda row: row.astype(str).str.count(\"Genetic\").sum(), axis=1)\n",
    "    df['ppi_physical_count'] = df.loc[:, ppi_type_features].apply(\n",
    "        lambda row: row.astype(str).str.count(\"Physical\").sum(), axis=1)\n",
    "    df['ppi_gen_phys_count'] = df.loc[:, ppi_type_features].apply(\n",
    "        lambda row: row.astype(str).str.count(\"Genetic-Physical\").sum(), axis=1)\n",
    "    \n",
    "    df['ppi_genetic_physical_ratio'] = df['ppi_genetic_count'] / (1 + df['ppi_physical_count'])\n",
    "    df['ppi_genetic_physical_diff'] = df['ppi_genetic_count'] - df['ppi_physical_count']\n",
    "    \n",
    "    ppi_type_feats = ['ppi_genetic_count', 'ppi_physical_count', 'ppi_gen_phys_count']\n",
    "    df['ppi_dom_type'] = df[ppi_type_feats].idxmax(axis='columns').astype('category')\n",
    "    \n",
    "    # Get proteins of highest correlation, since that is the point of this dataset\n",
    "    # Note that if all ties, we return the first index\n",
    "    df['ppi_max_corr'] = df[ppi_corr_features].idxmax(axis='columns').astype('category')\n",
    "    df['ppi_min_corr'] = df[ppi_corr_features].idxmax(axis='columns').astype('category')\n",
    "    df['strongest_localization'] = df['ppi_max_corr'].apply(ppi_to_localization).astype('category')\n",
    "\n",
    "    # This is the localization of the protein that has the strongest pos/neg corelation\n",
    "    cols = df.loc[:,ppi_corr_features].columns\n",
    "    gt = df.loc[:,ppi_corr_features].apply(lambda x: x > 0)\n",
    "    lt = df.loc[:,ppi_corr_features].apply(lambda x: x < 0)\n",
    "    nonzero = gt | lt  # < 0 or > 0\n",
    "\n",
    "    df['mode_localization_pos'] = gt.apply(\n",
    "        lambda x: localization_mode([ppi_to_localization(prot, 2960) for prot in cols[x.values]]),\n",
    "        axis=1).astype('category')\n",
    "    df['mode_localization_neg'] = lt.apply(\n",
    "        lambda x: localization_mode([ppi_to_localization(prot, 2960) for prot in cols[x.values]]),\n",
    "        axis=1).astype('category')\n",
    "    \n",
    "    # Second degree positive interaction values\n",
    "    df['second_deg_interaction_sum_pos'] = gt.apply(\n",
    "        lambda x: sum([ppi_to_localization(prot, 'interaction_sum') for prot in cols[x.values]]), axis=1)\n",
    "    df['second_deg_interaction_mean_pos'] = gt.apply(\n",
    "        lambda x: my_mean([ppi_to_localization(prot, 'interaction_mean') for prot in cols[x.values]]), axis=1).fillna(0)\n",
    "    df['second_deg_interaction_max_pos'] = gt.apply(\n",
    "        lambda x: max([ppi_to_localization(prot, 'interaction_max') for prot in cols[x.values]], default=0), axis=1)\n",
    "\n",
    "    # Second degree negative interaction values\n",
    "    df['second_deg_interaction_sum_neg'] = lt.apply(\n",
    "        lambda x: sum([ppi_to_localization(prot, 'interaction_sum') for prot in cols[x.values]]), axis=1)\n",
    "    df['second_deg_interaction_mean_neg'] = lt.apply(\n",
    "        lambda x: my_mean([ppi_to_localization(prot, 'interaction_mean') for prot in cols[x.values]]), axis=1).fillna(0)\n",
    "    df['second_deg_interaction_max_neg'] = lt.apply(\n",
    "        lambda x: max([ppi_to_localization(prot, 'interaction_max') for prot in cols[x.values]], default=0), axis=1)\n",
    "\n",
    "    # Second degree negative interaction values\n",
    "    df['second_deg_interaction_sum_neg'] = lt.apply(\n",
    "        lambda x: sum([ppi_to_localization(prot, 'interaction_sum') for prot in cols[x.values]]), axis=1)\n",
    "    df['second_deg_interaction_mean_neg'] = lt.apply(\n",
    "        lambda x: np.mean([ppi_to_localization(prot, 'interaction_mean') for prot in cols[x.values]]), axis=1).fillna(0)\n",
    "    df['second_deg_interaction_max_neg'] = lt.apply(\n",
    "        lambda x: sum([ppi_to_localization(prot, 'interaction_max') for prot in cols[x.values]]), axis=1)\n",
    "    \n",
    "    # Second Degree Motif Interactions of positive correlation\n",
    "    # reminder: gt is the indicator dataframe of positive interaction correlations \n",
    "    df['second_deg_motif_interaction_sum_pos'] = gt.apply(\n",
    "        lambda x: sum([ppi_to_localization(prot, 'motif_interactions') for prot in cols[x.values]]), axis=1)\n",
    "    df['second_deg_motif_interaction_mean_pos'] = gt.apply(\n",
    "        lambda x: my_mean([ppi_to_localization(prot, 'interaction_mean') for prot in cols[x.values]]), axis=1).fillna(0)\n",
    "    df['second_deg_motif_interaction_max_pos'] = gt.apply(\n",
    "        lambda x: max([ppi_to_localization(prot, 'interaction_max') for prot in cols[x.values]], default=0), axis=1)\n",
    "\n",
    "    # Second Degree Motif Interactions of negative correlation\n",
    "    # reminder: lt is the indicator dataframe of negative interaction correlations \n",
    "    df['second_deg_motif_interaction_sum_neg'] = lt.apply(\n",
    "        lambda x: sum([ppi_to_localization(prot, 'motif_interactions') for prot in cols[x.values]]), axis=1)\n",
    "    df['second_deg_motif_interaction_mean_neg'] = lt.apply(\n",
    "        lambda x: my_mean([ppi_to_localization(prot, 'interaction_mean') for prot in cols[x.values]]), axis=1).fillna(0)\n",
    "    df['second_deg_motif_interaction_max_neg'] = lt.apply(\n",
    "        lambda x: max([ppi_to_localization(prot, 'interaction_max') for prot in cols[x.values]], default=0), axis=1)\n",
    "\n",
    "    # Second Degree Motif Interactions of nonzero correlation\n",
    "    # reminder: lt is the indicator dataframe of negative interaction correlations \n",
    "    df['second_deg_motif_interaction_sum'] = nonzero.apply(\n",
    "        lambda x: sum([ppi_to_localization(prot, 'motif_interactions') for prot in cols[x.values]]), axis=1)\n",
    "    df['second_deg_motif_interaction_mean'] = nonzero.apply(\n",
    "        lambda x: my_mean([ppi_to_localization(prot, 'interaction_mean') for prot in cols[x.values]]), axis=1).fillna(0)\n",
    "    df['second_deg_motif_interaction_max'] = nonzero.apply(\n",
    "        lambda x: max([ppi_to_localization(prot, 'interaction_max') for prot in cols[x.values]], default=0), axis=1)\n",
    "    \n",
    "    # For positive correlations, let's create a column for 14 classes + unknown\n",
    "    ppis_gt = gt.apply(lambda x: [ppi_to_localization(prot, 2960) for prot in cols[x.values]], axis=1)\n",
    "    ppi_pos_pct_cols = [f\"ppi_pos_pct_{i}\" for i in range(14)]\n",
    "    df_ppi_pos_pct = pd.DataFrame(ppis_gt.apply(arr_percents).tolist(), columns=ppi_pos_pct_cols)\n",
    "    df = pd.concat((df, df_ppi_pos_pct), axis=1)\n",
    "    \n",
    "    # For negative correlations, let's create a column for 14 classes + unknown\n",
    "    ppis_lt = lt.apply(lambda x: [ppi_to_localization(prot, 2960) for prot in cols[x.values]], axis=1)\n",
    "    ppi_neg_pct_cols = [f\"ppi_neg_pct_{i}\" for i in range(14)]\n",
    "    df_ppi_neg_pct = pd.DataFrame(ppis_lt.apply(arr_percents).tolist(), columns=ppi_neg_pct_cols)\n",
    "    df = pd.concat((df, df_ppi_neg_pct), axis=1)\n",
    "    \n",
    "    # For ALL correlations, let's create a column for 14 classes + unknown\n",
    "    ppis = nonzero.apply(lambda x: [ppi_to_localization(prot, 2960) for prot in cols[x.values]], axis=1)\n",
    "    ppi_pct_cols = [f\"ppi_all_pct_{i}\" for i in range(14)]\n",
    "    df_ppi_all_pct = pd.DataFrame(ppis.apply(arr_percents).tolist(), columns=ppi_pct_cols)\n",
    "    df = pd.concat((df, df_ppi_all_pct), axis=1)\n",
    "    \n",
    "    # Neighbour intraclass correlation features\n",
    "    # Here I create a vector whose elements are a dict\n",
    "    # This is a dict that gives the (row_index, corr_feature_column) : class of that feature\n",
    "    nbr_classes = nonzero.apply(lambda x: (\n",
    "        x.name, {prot : ppi_to_localization(prot, 2960) for prot in cols[x.values]}), axis=1)\n",
    "\n",
    "    # Okay now we can generate these features that are functions\n",
    "    # of correlations of the neighbours broken down by class\n",
    "    df_nbr_corr_sum = nbr_classes.apply(lambda row: class_corr_feat(row, func=sum))\n",
    "    df_nbr_corr_mean = nbr_classes.apply(lambda row: class_corr_feat(row, func=np.mean))\n",
    "    df_nbr_corr_max = nbr_classes.apply(lambda row: class_corr_feat(row, func=lambda vals: max(vals, default=0)))\n",
    "    df_nbr_corr_std = nbr_classes.apply(lambda row: class_corr_feat(row, func=np.std))\n",
    "    df_nbr_corr_skew = nbr_classes.apply(\n",
    "        lambda row: class_corr_feat(row, func=lambda vals: sp.stats.skew(vals, nan_policy='omit')))\n",
    "    \n",
    "    df_nbr_corr_sum = pd.DataFrame(df_nbr_corr_sum.tolist(), columns=[f\"nbr_corr_sum_{i}\" for i in range(14)])\n",
    "    df_nbr_corr_mean = pd.DataFrame(df_nbr_corr_mean.tolist(), columns=[f\"nbr_corr_mean_{i}\" for i in range(14)])\n",
    "    df_nbr_corr_max = pd.DataFrame(df_nbr_corr_max.tolist(), columns=[f\"nbr_corr_max_{i}\" for i in range(14)])\n",
    "    df_nbr_corr_std = pd.DataFrame(df_nbr_corr_std.tolist(), columns=[f\"nbr_corr_std_{i}\" for i in range(14)])\n",
    "    df_nbr_corr_skew = pd.DataFrame(df_nbr_corr_skew.tolist(), columns=[f\"nbr_corr_skew_{i}\" for i in range(14)])\n",
    "    df = pd.concat((\n",
    "        df,\n",
    "        df_nbr_corr_sum,\n",
    "        df_nbr_corr_mean,\n",
    "        df_nbr_corr_max,\n",
    "        df_nbr_corr_std,\n",
    "        df_nbr_corr_skew,\n",
    "    ), axis=1)\n",
    "    \n",
    "    # Drop Function Features\n",
    "    function_feats_qry = fields[0].str.contains(\"function\")\n",
    "    function_feats = fields[[0]][function_feats_qry].index\n",
    "    function_feats = set(function_feats[function_feats > 2900])\n",
    "    \n",
    "    # Use only selected features\n",
    "    X = df[set(df.columns) - {target_col, 0, 2959} - ppi_features - function_feats]\n",
    "    if target_col is not None:\n",
    "        y = df[target_col]\n",
    "    \n",
    "    # Return Datasets\n",
    "    if target_col is not None:\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Pipeline to Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert True  # Stop running notebook here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angus/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (444) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df_original = pd.read_csv(f\"{data_fpath}train.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{out_fpath}data_types_dict.pkl', 'rb') as handle:\n",
    "    dtypes = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angus/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/angus/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/angus/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:261: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 s, sys: 202 ms, total: 11.4 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X, y = data_pipeline(df_original, fields, dtypes, impute=True, target_col=2960, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 862 entries, 0 to 861\n",
      "Columns: 610 entries, 1 to nbr_corr_skew_3\n",
      "dtypes: category(450), float64(135), int64(25)\n",
      "memory usage: 1.5 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Pipeline to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angus/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (524,720,768,1112,1182,1288,1302,1352,1354,1378,1434,1436,1488,1502,1504,1604,1608,1734,1838,1908,1914,1942,1996,2246,2270,2328,2460,2514,2576,2620,2724,2758,2930) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.read_csv(f\"{data_fpath}test.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{out_fpath}data_types_dict.pkl', 'rb') as handle:\n",
    "    dtypes = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-cd211ea44784>:17: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if not target_value:\n",
      "/Users/angus/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/angus/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/angus/anaconda3/lib/python3.8/site-packages/numpy/core/_methods.py:261: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 124 ms, total: 11.2 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_kaggle = data_pipeline(testdf, fields, dtypes, target_col=None, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 381 entries, 0 to 380\n",
      "Columns: 610 entries, 1 to nbr_corr_skew_3\n",
      "dtypes: category(450), float64(135), int64(25)\n",
      "memory usage: 718.1 KB\n"
     ]
    }
   ],
   "source": [
    "X_kaggle.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_tools.plot_train_test_feats(X, X_kaggle, 'mode_localization_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X.columns) - len(set(X.columns)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_pickle(f\"{out_fpath}X.pkl\")\n",
    "y.to_pickle(f\"{out_fpath}y.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kaggle.to_pickle(f\"{out_fpath}X_kaggle.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For One-Hot Encoding\n",
    "* need to concatenate the kaggle dataframe first, since not all categories are present in both training and kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1243, 610)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First concat X and competition data\n",
    "combined = pd.concat((X, X_kaggle), axis=0)\n",
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1243 entries, 0 to 380\n",
      "Columns: 610 entries, 1 to nbr_corr_skew_3\n",
      "dtypes: category(197), float64(135), int64(27), object(251)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some datatypes have been altered. Let's coerce them back to the correct ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_dict = X.dtypes.astype(str).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1243 entries, 0 to 380\n",
      "Columns: 610 entries, 1 to nbr_corr_skew_3\n",
      "dtypes: category(450), float64(135), int64(25)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "combined = combined.astype(dtypes_dict)\n",
    "combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1243 entries, 0 to 380\n",
      "Columns: 2112 entries, 445 to ppi_min_corr_2944\n",
      "dtypes: float64(135), int64(25), uint8(1952)\n",
      "memory usage: 3.8 MB\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode\n",
    "categoricals = combined.select_dtypes(include=['category']).columns\n",
    "combined_enc = pd.get_dummies(data=combined, columns=categoricals, drop_first=False)\n",
    "combined_enc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Let's split it back up into X_enc, X_kaggle_enc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 862 entries, 0 to 861\n",
      "Columns: 2112 entries, 445 to ppi_min_corr_2944\n",
      "dtypes: float64(135), int64(25), uint8(1952)\n",
      "memory usage: 2.7 MB\n"
     ]
    }
   ],
   "source": [
    "X_enc = combined_enc.iloc[:X.shape[0]]\n",
    "X_enc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 381 entries, 0 to 380\n",
      "Columns: 2112 entries, 445 to ppi_min_corr_2944\n",
      "dtypes: float64(135), int64(25), uint8(1952)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "X_kaggle_enc = combined_enc.iloc[X.shape[0]:]\n",
    "X_kaggle_enc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving output (one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enc.to_pickle(f\"{out_fpath}X_enc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kaggle_enc.to_pickle(f\"{out_fpath}X_kaggle_enc.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
